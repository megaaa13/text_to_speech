{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV2TTS Tacotron-2 single-speaker fine-tuning\n",
    "\n",
    "The main advantage of the `SV2TTS` is that it allows really fast training on single-speaker with few data.\n",
    "\n",
    "This notebook shows how to fine-tune a pretrained `multi-speaker SV2TTS` on `single-speaker` based on an `identification dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports + model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version : 2.3.2\n",
      "Available GPU's : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from models.tts import SV2TTSTacotron2, PtWaveGlow\n",
    "from custom_architectures import get_architecture\n",
    "from datasets import get_dataset, train_test_split, filter_dataset\n",
    "from utils import plot_spectrogram, select_embedding, add_speaker_embedding\n",
    "from utils.text import default_french_encoder\n",
    "from utils.audio import display_audio, load_audio, embed_annotation_dataset\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "rate = 22050\n",
    "model_name = 'sv2tts_fine_tuned'\n",
    "\n",
    "print(\"Tensorflow version : {}\".format(tf.__version__))\n",
    "print(\"Available GPU's : {}\".format(gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restoration...\n",
      "Initializing submodel : tts_model !\n",
      "Optimizer 'tts_model_optimizer' initilized successfully !\n",
      "Submodel tts_model compiled !\n",
      "  Loss : {'reduction': 'none', 'name': 'tacotron_loss', 'mel_loss': 'mse', 'mask_mel_padding': True, 'label_smoothing': 0, 'finish_weight': 1.0, 'not_finish_weight': 1.0, 'from_logits': False}\n",
      "  Optimizer : {'name': 'Adam', 'learning_rate': 0.0005000000237487257, 'decay': 0.0, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'epsilon': 1e-07, 'amsgrad': False}\n",
      "  Metrics : []\n",
      "Successfully restored tts_model from pretrained_models/sv2tts_tacotron2_256/saving/tts_model.json !\n",
      "Model sv2tts_tacotron2_256 initialized successfully !\n",
      "Initializing submodel : tts_model !\n",
      "Submodel tts_model saved in pretrained_models\\sv2tts_fine_tuned\\saving\\tts_model.json !\n",
      "Model sv2tts_fine_tuned initialized successfully !\n",
      "Weights transfered successfully !\n",
      "Submodel tts_model saved in pretrained_models\\sv2tts_fine_tuned\\saving\\tts_model.json !\n",
      "\n",
      "========== sv2tts_fine_tuned ==========\n",
      "Sub model tts_model\n",
      "- Inputs \t: unknown\n",
      "- Outputs \t: unknown\n",
      "- Number of layers \t: 3\n",
      "- Number of parameters \t: 30.341 Millions\n",
      "- Model not compiled\n",
      "\n",
      "Already trained on 0 epochs (0 steps)\n",
      "\n",
      "Input language : fr\n",
      "Input vocab (size = 148) : ['_', '-', '!', \"'\", '(', ')', ',', '.', ':', ';', '?', ' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'é', 'è', 'ê', 'î', 'ç', 'ô', 'ù', 'ukn_0', 'ukn_1', 'ukn_2', 'ukn_3', 'ukn_4', 'ukn_5', 'ukn_6', 'ukn_7', 'ukn_8', 'ukn_9', 'ukn_10', 'ukn_11', 'ukn_12', 'ukn_13', 'ukn_14', 'ukn_15', 'ukn_16', 'ukn_17', 'ukn_18', 'ukn_19', 'ukn_20', 'ukn_21', 'ukn_22', 'ukn_23', 'ukn_24', 'ukn_25', 'ukn_26', 'ukn_27', 'ukn_28', 'ukn_29', 'ukn_30', 'ukn_31', 'ukn_32', 'ukn_33', 'ukn_34', 'ukn_35', 'ukn_36', 'ukn_37', 'ukn_38', 'ukn_39', 'ukn_40', 'ukn_41', 'ukn_42', 'ukn_43', 'ukn_44', 'ukn_45', 'ukn_46', 'ukn_47', 'ukn_48', 'ukn_49', 'ukn_50', 'ukn_51', 'ukn_52', 'ukn_53', 'ukn_54', 'ukn_55', 'ukn_56', 'ukn_57', 'ukn_58', 'ukn_59', 'ukn_60', 'ukn_61', 'ukn_62', 'ukn_63', 'ukn_64', 'ukn_65', 'ukn_66', 'ukn_67', 'ukn_68', 'ukn_69', 'ukn_70', 'ukn_71', 'ukn_72', 'ukn_73', 'ukn_74', 'ukn_75']\n",
      "Audio rate : 22050\n",
      "Mel channels : 80\n",
      "Speaker embedding dim : 256\n",
      "Speaker encoder : audio_siamese_256_mel_lstm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SV2TTSTacotron2.build_from_sv2tts_pretrained(pretrained_name = 'sv2tts_tacotron2_256', nom = model_name, compile = False)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SV2TTSTacotron2(nom = model_name)\n",
    "\n",
    "lr = { 'name': 'WarmupScheduler', 'maxval' : 5e-4, 'minval' : 25e-5, 'factor' : 256, 'warmup_steps' : 256}\n",
    "\n",
    "model.compile(optimizer = 'adam', optimizer_config = {'lr' : lr})\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation / loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/datasets/...'\n",
    "\n",
    "dataset = embed_annotation_dataset(\n",
    "    path, embed_fn = model.speaker_encoder.embed, embedding_dim = model.embedding_dim, rate = model.speaker_encoder.audio_rate,\n",
    "    embedding_name = 'embeddings_256_mel_lstm.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset SAO...\n",
      "Dataset length : 489 (1 speakers)\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    'directory'      : 'D:/datasets/...',\n",
    "    'type_annots'    : 'identification',\n",
    "    'embedding_dim'  : model.speaker_embedding_dim,\n",
    "    'embedding_name' : 'embeddings_256_mel_lstm.csv'\n",
    "}\n",
    "\n",
    "dataset = get_dataset(\n",
    "    '...', ds_type = 'custom', ** kwargs\n",
    ")\n",
    "\n",
    "dataset = filter_dataset(dataset,id = '...')\n",
    "dataset.pop('indexes')\n",
    "\n",
    "print(\"Dataset length : {} ({} speakers)\".format(\n",
    "    len(dataset), len(dataset['id'].unique())\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs          = 15\n",
    "batch_size      = 16\n",
    "valid_batch_size    = batch_size\n",
    "\n",
    "max_valid_size  = min(int(0.1 * len(dataset)), 256 * valid_batch_size)\n",
    "\n",
    "train_size      = min(1024 * batch_size, len(dataset) - max_valid_size)\n",
    "valid_size      = min(len(dataset) - train_size, max_valid_size)\n",
    "\n",
    "shuffle_size    = batch_size * 8\n",
    "pred_step       = -1\n",
    "\n",
    "\"\"\" Custom training hparams \"\"\"\n",
    "augment_prct        = 0.2\n",
    "augment_speaker_embedding   = False\n",
    "\n",
    "trim_audio      = True\n",
    "reduce_noise    = False\n",
    "trim_threshold  = 0.025\n",
    "max_silence     = 0.1\n",
    "trim_method     = 'remove'\n",
    "trim_mode       = 'start_end'\n",
    "\n",
    "trim_mel     = False\n",
    "trim_factor  = 0.6\n",
    "trim_mel_method  = 'max_start_end'\n",
    "\n",
    "# Seems to be interesting for single-speaker fine-tuning\n",
    "# and for a better generalization but seems to slow down convergence \n",
    "use_utterance_embedding = True\n",
    "\n",
    "max_input_length = 300\n",
    "max_output_length = 2048\n",
    "\n",
    "\"\"\" Training \"\"\"\n",
    "\n",
    "train, valid = train_test_split(\n",
    "    dataset, train_size = train_size, valid_size = valid_size, shuffle = True\n",
    ")\n",
    "\n",
    "print(\"Training samples   : {} - {} batches - {} speakers\".format(\n",
    "    len(train), len(train) // batch_size, len(train['id'].unique())\n",
    "))\n",
    "print(\"Validation samples : {} - {} batches - {} speakers\".format(\n",
    "    len(valid), len(valid) // valid_batch_size, len(valid['id'].unique())\n",
    "))\n",
    "\n",
    "# This feature seems interesting for singl-speaker fine-tuning\n",
    "# If you want to enable it, put `trainable = False`\n",
    "trainable = False\n",
    "if model.epochs >= 5:\n",
    "    model.tts_model.postnet.trainable = trainable\n",
    "if model.epochs >= 10:\n",
    "    model.tts_model.encoder.trainable = trainable\n",
    "\n",
    "model.train(\n",
    "    train, validation_data = valid, \n",
    "    epochs = epochs, batch_size = batch_size, valid_batch_size = valid_batch_size,\n",
    "\n",
    "    max_input_length = max_input_length, max_output_length = max_output_length,\n",
    "    shuffle_size = shuffle_size, pred_step = pred_step,\n",
    "    augment_prct = augment_prct, augment_speaker_embedding = augment_speaker_embedding,\n",
    "    \n",
    "    trim_audio = trim_audio, reduce_noise = reduce_noise, trim_threshold = trim_threshold,\n",
    "    max_silence = max_silence, trim_method = trim_method, trim_mode = trim_mode,\n",
    "    \n",
    "    trim_mel = trim_mel, trim_factor = trim_factor, trim_mel_method = trim_mel_method,\n",
    "    \n",
    "    use_utterance_embedding = use_utterance_embedding\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete inference\n",
    "\n",
    "These cells allow you to test your model with a complete inference pipeline\n",
    "\n",
    "Note that you have to restart your kernel then execute 1st cell (imports) then cells below. You **must** first instanciate the `PtWaveGlow` model which will call `limit_gpu_memory` to reduce visible GPU memory for tensorflow (to allow a better coexistance of both libraries). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveglow = PtWaveGlow()\n",
    "model    = SV2TTSTacotron2(nom = model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_inference(text, embedding, n = 1):\n",
    "    encoded = tf.expand_dims(model.encode_text(text), axis = 0)\n",
    "    \n",
    "    _, mel, _, attn = model.infer(\n",
    "        encoded, [tf.shape(encoded)[1]], embedding\n",
    "    )\n",
    "    \n",
    "    mel = np.squeeze(mel, 0)\n",
    "    \n",
    "    plot_spectrogram(inference = mel, attention = attn)\n",
    "    audio = waveglow.infer(mel)\n",
    "\n",
    "    display_audio(audio, rate = rate)\n",
    "    return audio\n",
    "\n",
    "kwargs = {\n",
    "    'directory'      : 'D:/datasets/...',\n",
    "    'type_annots'    : 'identification',\n",
    "    'embedding_dim'  : model.speaker_embedding_dim,\n",
    "    'embedding_name' : 'embeddings_256_mel_lstm.csv'\n",
    "}\n",
    "\n",
    "dataset = get_dataset(\n",
    "    '...', ds_type = 'custom', ** kwargs\n",
    ")\n",
    "\n",
    "dataset = filter_dataset(dataset, id = '...')\n",
    "dataset = add_speaker_embedding(dataset, 'emotion', 'mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Bonjour tout le monde ! Voici une démonstration du modèle en français.\"\n",
    "\n",
    "x = random.randrange(0, len(dataset))\n",
    "display_audio(dataset.at[x, 'filename'])\n",
    "embedding = select_embedding(dataset, mode = x)\n",
    "\n",
    "silence = np.zeros((int(rate * 0.15),))\n",
    "audios = []\n",
    "if not isinstance(text, list): text = [text]\n",
    "\n",
    "for p in text:\n",
    "    for _ in range(2):\n",
    "        audio = full_inference(p, embedding)\n",
    "    audios.append(audio)\n",
    "    audios.append(silence)\n",
    "\n",
    "if len(text) > 1:\n",
    "    audios = np.concatenate(audios)\n",
    "    _ = display_audio(audios, rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waveglow inference on training generated audios\n",
    "\n",
    "This is a demonstration on prediction at step 500 so it is normal that inference is so bad ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_target(model_name, step, n, mode, save = False, display = True):\n",
    "    if mode == 'train':\n",
    "        directory = os.path.join('pretrained_models', model_name, 'training-logs', 'eval', 'mels')\n",
    "        filename = 'pred_step-{:06d}_{}_target.npy'.format(step, n)\n",
    "        pred_filename = 'pred_step-{:06d}_{}_pred.npy'.format(step, n)\n",
    "        infer_filename = 'pred_step-{:06d}_{}_infer.npy'.format(step, n)\n",
    "    else:\n",
    "        directory = os.path.join('pretrained_models', model_name, 'outputs', 'mels')\n",
    "        filename = 'pred_{}_target.npy'.format(n)\n",
    "        pred_filename = 'pred_{}_pred.npy'.format(n)\n",
    "        infer_filename = 'pred_{}_infer.npy'.format(n)\n",
    "\n",
    "    if not os.path.exists(os.path.join(directory, filename)): return\n",
    "    \n",
    "    target = np.load(os.path.join(directory, filename))\n",
    "    pred   = np.load(os.path.join(directory, pred_filename))\n",
    "    infer  = np.load(os.path.join(directory, infer_filename))\n",
    "    \n",
    "    audio       = waveglow.infer(target)\n",
    "    audio_pred  = waveglow.infer(pred)\n",
    "    audio_infer = waveglow.infer(infer)\n",
    "\n",
    "    _ = display_audio(audio, rate = rate)\n",
    "    _ = display_audio(audio_pred, rate = rate)\n",
    "    _ = display_audio(audio_infer, rate = rate)\n",
    "    \n",
    "    if save:\n",
    "        save_dir = directory.replace('mels', 'audios')\n",
    "        os.makedirs(save_dir, exist_ok = True)\n",
    "        write_audio(audio, os.path.join(save_dir, filename[:-3] + 'mp3'), rate = rate)\n",
    "        write_audio(audio_pred, os.path.join(save_dir, pred_filename[:-3] + 'mp3'), rate = rate)\n",
    "        write_audio(audio_infer, os.path.join(save_dir, infer_filename[:-3] + 'mp3'), rate = rate)\n",
    "    \n",
    "    plot_spectrogram(\n",
    "        target = target, prediction = pred, inference = infer\n",
    "    )\n",
    "\n",
    "waveglow = get_architecture('nvidia_waveglow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, mode = 500, 'train'\n",
    "\n",
    "for n in range(5):\n",
    "    infer_with_target(model_name, step, n, mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_train_objects.optimizers import WarmupScheduler\n",
    "\n",
    "lr = WarmupScheduler(maxval = 75e-5, minval = 25e-5, factor = 1024)\n",
    "print(lr.get_config())\n",
    "lr.plot(1024 * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
