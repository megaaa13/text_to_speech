{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV2TTS Tacotron-2 training\n",
    "\n",
    "This notebook shows how to create (with *partial transfer learning*), train and use the `SV2TTS` architecture (see the `README` file for more information)\n",
    "\n",
    "The structure of this notebook is exactly the same as `example_tacotron2`, check it if you want more details on each part ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports + model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version : 2.6.2\n",
      "Available GPU's : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from models.tts import SV2TTSTacotron2, WaveGlow\n",
    "from custom_architectures import get_architecture\n",
    "from datasets import get_dataset, train_test_split\n",
    "from utils import plot_spectrogram, select_embedding, limit_gpu_memory\n",
    "from utils.text import default_french_encoder\n",
    "from utils.audio import display_audio, load_audio\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "rate = 22050\n",
    "model_name = 'sv2tts_tacotron2_256'\n",
    "\n",
    "print(\"Tensorflow version : {}\".format(tf.__version__))\n",
    "print(\"Available GPU's : {}\".format(gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This special cleaner allow to not lowercase the text \n",
    "# see my data_processing repository for more examples on text encoding / cleaners\n",
    "# If you want lowercase, you just have to remove the \"cleaners\" argument from default_french_encoder()\n",
    "cleaners = [\n",
    "    'fr_convert_to_ascii',\n",
    "    {'name' : 'expand_numbers', 'langue' : 'fr'},\n",
    "    'collapse_whitespace'\n",
    "]\n",
    "\n",
    "encoder = default_french_encoder(vocab_size = 148, cleaners = cleaners)\n",
    "print(encoder)\n",
    "\n",
    "config = {\n",
    "    'nom' : model_name,\n",
    "    'lang' : 'fr',\n",
    "    'text_encoder' : encoder,\n",
    "    'speaker_encoder_name'  : 'audio_siamese_256_mel_lstm',\n",
    "    'speaker_embedding_dim' : 256,\n",
    "    'use_utterance_embedding' : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SV2TTSTacotron2.from_nvidia_pretrained(** config)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SV2TTSTacotron2.from_pretrained(pretrained_name = 'tacotron2_siwis', ** config)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_name = 'sv2tts_tacotron2_256'\n",
    "\n",
    "model = SV2TTSTacotron2.from_pretrained(\n",
    "    pretrained_name = pretrained_name, ** config\n",
    ")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization + dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SV2TTSTacotron2(nom = model_name)\n",
    "\n",
    "lr = { 'name': 'WarmupScheduler', 'maxval' : 75e-5, 'minval' : 25e-5, 'factor' : 1024, 'warmup_steps' : 2048}\n",
    "lr = 5e-4\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam', optimizer_config = {'lr' : lr}\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'embedding_dim'  : model.speaker_embedding_dim,\n",
    "    'embedding_name' : 'embeddings_256_mel_lstm.csv'\n",
    "}\n",
    "\n",
    "datasets = ['siwis', 'voxforge', 'common_voice']\n",
    "datasets = ['siwis', 'voxforge']\n",
    "\n",
    "dataset = get_dataset(\n",
    "    datasets, accent = 'france', shuffle = True, ** kwargs\n",
    ")\n",
    "# If the dataset is a dict, it is typically a STT dataset where\n",
    "# validation set contains \"new speakers\" but it is not relevant for this model\n",
    "# because generalization to new speakers is really hard and only interesting if datasets\n",
    "# have many speakers (>> 1k) (which is really rare in TTS datasets)\n",
    "if isinstance(dataset, dict): dataset = dataset['train']\n",
    "#dataset = filter_dataset(dataset, duree = lambda d: d < 11)\n",
    "\n",
    "print(\"Dataset length : {} ({} speakers)\".format(\n",
    "    len(dataset), len(dataset['id'].unique())\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model requires `text`, `filename` and `embedding` (or `speaker_embedding`) columns. The `wavs_22050` allows to directly load resampled audio (which speeds up training). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs          = 5\n",
    "batch_size      = 32\n",
    "valid_batch_size    = batch_size\n",
    "\n",
    "max_valid_size  = min(int(0.1 * len(dataset)), 256 * valid_batch_size)\n",
    "\n",
    "train_size      = min(1024 * batch_size, len(dataset) - max_valid_size)\n",
    "valid_size      = min(len(dataset) - train_size, max_valid_size)\n",
    "\n",
    "shuffle_size    = batch_size * 12\n",
    "pred_step       = 512\n",
    "\n",
    "\"\"\" Custom training hparams \"\"\"\n",
    "augment_prct        = 0.25\n",
    "augment_speaker_embedding   = False\n",
    "\n",
    "trim_audio      = True\n",
    "reduce_noise    = True if 'common_voice' in datasets else False\n",
    "trim_threshold  = 0.075\n",
    "max_silence     = 0.1\n",
    "trim_method     = 'window'\n",
    "trim_mode       = 'start_end'\n",
    "\n",
    "trim_mel     = False\n",
    "trim_factor  = 0.6\n",
    "trim_mel_method  = 'max_start_end'\n",
    "\n",
    "# Seems to be interesting for single-speaker fine-tuning\n",
    "# and for a better generalization but seems to slow down convergence \n",
    "use_utterance_embedding = True\n",
    "\n",
    "max_input_length = 75\n",
    "max_output_length = 512\n",
    "\n",
    "\"\"\" Training \"\"\"\n",
    "\n",
    "train, valid = train_test_split(\n",
    "    dataset, train_size = train_size, valid_size = valid_size, shuffle = True\n",
    ")\n",
    "\n",
    "print(\"Training samples   : {} - {} batches - {} speakers\".format(\n",
    "    len(train), len(train) // batch_size, len(train['id'].unique())\n",
    "))\n",
    "print(\"Validation samples : {} - {} batches - {} speakers\".format(\n",
    "    len(valid), len(valid) // valid_batch_size, len(valid['id'].unique())\n",
    "))\n",
    "\n",
    "# This feature seems interesting for singl-speaker fine-tuning\n",
    "# If you want to enable it, put `trainable = False`\n",
    "trainable = False\n",
    "if model.epochs >= 5:\n",
    "    model.tts_model.postnet.trainable = trainable\n",
    "if model.epochs >= 10:\n",
    "    model.tts_model.encoder.trainable = trainable\n",
    "\n",
    "model.train(\n",
    "    train, validation_data = valid, \n",
    "    epochs = epochs, batch_size = batch_size, valid_batch_size = valid_batch_size,\n",
    "\n",
    "    max_input_length = max_input_length, max_output_length = max_output_length,\n",
    "    shuffle_size = shuffle_size, pred_step = pred_step,\n",
    "    augment_prct = augment_prct, augment_speaker_embedding = augment_speaker_embedding,\n",
    "    \n",
    "    trim_audio = trim_audio, reduce_noise = reduce_noise, trim_threshold = trim_threshold,\n",
    "    max_silence = max_silence, trim_method = trim_method, trim_mode = trim_mode,\n",
    "    \n",
    "    trim_mel = trim_mel, trim_factor = trim_factor, trim_mel_method = trim_mel_method,\n",
    "    \n",
    "    use_utterance_embedding = use_utterance_embedding\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_history()\n",
    "print(model.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction / inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with target\n",
    "\n",
    "This cell allows you to make prediction **with** target : it means the model will have an inference and a prediction (as during training)\n",
    "\n",
    "Next you can make the `waveglow inference` with the section related to `prediction on training generated` (you just have to replace `mode = 'train'` by `mode = 'pred'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.get_dataset_config(batch_size = 1, cache = False, is_validation = True)\n",
    "\n",
    "sub_ds = valid.sample(15, random_state = 1024)\n",
    "ds = prepare_dataset(sub_ds, ** config)\n",
    "\n",
    "for i, batch in enumerate(ds):\n",
    "    model.predict_with_target(\n",
    "        batch, step = 'test', prefix = 'pred_{}'.format(i),\n",
    "        directory = model.pred_dir, show = True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction without target\n",
    "\n",
    "This cell will allow you to make prediction **without** target (ie by putting the text you want) !\n",
    "\n",
    "Note that this 1st cell is to set the default embeddings to use in prediction : by default, it samples the training dataset to define default embeddings. These default embeddings are the one used when prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'embedding_dim'  : model.speaker_embedding_dim,\n",
    "    'embedding_name' : 'embeddings_256_mel_lstm.csv'\n",
    "}\n",
    "\n",
    "datasets = 'siwis'\n",
    "\n",
    "dataset = get_dataset(datasets, ** kwargs)\n",
    "\n",
    "embeddings = dataset.sample(10)\n",
    "model.set_default_embeddings(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Bonjour le monde !\"\n",
    "\n",
    "model.predict(text, overwrite = True, save = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete inference\n",
    "\n",
    "These cells allow you to test your model with a complete inference pipeline\n",
    "\n",
    "Note that if you want to use `PtWaveGlow` instead of `WaveGlow`, you have to restart your kernel then execute 1st cell (imports) then cells below. You **must** first instanciate the `PtWaveGlow` model which will call `limit_gpu_memory` to reduce visible GPU memory for tensorflow (to allow a better coexistance of both libraries). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveglow = WaveGlow() #PtWaveGlow()\n",
    "model    = SV2TTSTacotron2(nom = model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_inference(text, embedding, n = 1):\n",
    "    encoded = tf.expand_dims(model.encode_text(text), axis = 0)\n",
    "    \n",
    "    _, mel, _, attn = model.infer(\n",
    "        encoded, [tf.shape(encoded)[1]], embedding\n",
    "    )\n",
    "    \n",
    "    plot_spectrogram(inference = mel[0], attention = attn)\n",
    "    audio = waveglow.infer(mel)\n",
    "\n",
    "    display_audio(audio, rate = rate)\n",
    "    return audio\n",
    "\n",
    "kwargs = {\n",
    "    'embedding_dim'  : model.speaker_embedding_dim,\n",
    "    'embedding_name' : 'embeddings_256_mel_lstm.csv'\n",
    "}\n",
    "\n",
    "datasets = ['siwis', 'voxforge']\n",
    "\n",
    "dataset = get_dataset(\n",
    "    datasets, accent = 'france', shuffle = True, ** kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Bonjour tout le monde ! Voici une démonstration du modèle en français.\"\n",
    "\n",
    "# Select an embedding and display original speaker audio associated to this embedding\n",
    "x = random.randrange(0, len(dataset))\n",
    "display_audio(dataset.at[x, 'filename'])\n",
    "embedding = select_embedding(dataset, mode = x)\n",
    "\n",
    "silence = np.zeros((int(rate * 0.15),))\n",
    "audios = []\n",
    "if not isinstance(text, list): text = [text]\n",
    "\n",
    "for p in text:\n",
    "    # Making n times the same inference allow to check the model's stability\n",
    "    for _ in range(2):\n",
    "        audio = full_inference(p, embedding)\n",
    "    audios.append(audio)\n",
    "    audios.append(silence)\n",
    "\n",
    "if len(text) > 1:\n",
    "    audios = np.concatenate(audios)\n",
    "    _ = display_audio(audios, rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waveglow inference on training generated audios\n",
    "\n",
    "This is a demonstration on prediction at step 500 so it is normal that inference is so bad ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_target(model_name, step, n, mode, save = False, display = True):\n",
    "    if mode == 'train':\n",
    "        directory = os.path.join('pretrained_models', model_name, 'training-logs', 'eval', 'mels')\n",
    "        filename = 'pred_step-{:06d}_{}_target.npy'.format(step, n)\n",
    "        pred_filename = 'pred_step-{:06d}_{}_pred.npy'.format(step, n)\n",
    "        infer_filename = 'pred_step-{:06d}_{}_infer.npy'.format(step, n)\n",
    "    else:\n",
    "        directory = os.path.join('pretrained_models', model_name, 'outputs', 'mels')\n",
    "        filename = 'pred_{}_target.npy'.format(n)\n",
    "        pred_filename = 'pred_{}_pred.npy'.format(n)\n",
    "        infer_filename = 'pred_{}_infer.npy'.format(n)\n",
    "\n",
    "    if not os.path.exists(os.path.join(directory, filename)): return\n",
    "    \n",
    "    target = np.load(os.path.join(directory, filename))\n",
    "    pred   = np.load(os.path.join(directory, pred_filename))\n",
    "    infer  = np.load(os.path.join(directory, infer_filename))\n",
    "    \n",
    "    audio       = waveglow.infer(target)\n",
    "    audio_pred  = waveglow.infer(pred)\n",
    "    audio_infer = waveglow.infer(infer)\n",
    "\n",
    "    _ = display_audio(audio, rate = rate)\n",
    "    _ = display_audio(audio_pred, rate = rate)\n",
    "    _ = display_audio(audio_infer, rate = rate)\n",
    "    \n",
    "    if save:\n",
    "        save_dir = directory.replace('mels', 'audios')\n",
    "        os.makedirs(save_dir, exist_ok = True)\n",
    "        write_audio(audio, os.path.join(save_dir, filename[:-3] + 'mp3'), rate = rate)\n",
    "        write_audio(audio_pred, os.path.join(save_dir, pred_filename[:-3] + 'mp3'), rate = rate)\n",
    "        write_audio(audio_infer, os.path.join(save_dir, infer_filename[:-3] + 'mp3'), rate = rate)\n",
    "    \n",
    "    plot_spectrogram(\n",
    "        target = target, prediction = pred, inference = infer\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, mode = 500, 'train'\n",
    "\n",
    "for n in range(5):\n",
    "    infer_with_target(model_name, step, n, mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_train_objects.optimizers import WarmupScheduler\n",
    "\n",
    "lr = WarmupScheduler(maxval = 75e-5, minval = 25e-5, factor = 1024)\n",
    "print(lr.get_config())\n",
    "lr.plot(1024 * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
