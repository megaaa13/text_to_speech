{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV2TTS Tacotron-2 training\n",
    "\n",
    "This notebook shows how to create (with *partial transfer learning*), train and use the `SV2TTS` architecture (see the `README` file for more information)\n",
    "\n",
    "The structure of this notebook is exactly the same as `example_tacotron2`, check it if you want more details on each part ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports + model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version : 2.3.2\n",
      "Available GPU's : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from models.tts import SV2TTSTacotron2, WaveGlow\n",
    "from custom_architectures import get_architecture\n",
    "from datasets import get_dataset, train_test_split\n",
    "from utils import plot_spectrogram, select_embedding, limit_gpu_memory\n",
    "from utils.text import default_french_encoder\n",
    "from utils.audio import display_audio, load_audio\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "rate = 22050\n",
    "model_name = 'sv2tts_tacotron2_256'\n",
    "\n",
    "print(\"Tensorflow version : {}\".format(tf.__version__))\n",
    "print(\"Available GPU's : {}\".format(gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This special cleaner allow to not lowercase the text \n",
    "# see my data_processing repository for more examples on text encoding / cleaners\n",
    "# If you want lowercase, you just have to remove the \"cleaners\" argument from default_french_encoder()\n",
    "cleaners = [\n",
    "    'fr_convert_to_ascii',\n",
    "    {'name' : 'expand_numbers', 'langue' : 'fr'},\n",
    "    'collapse_whitespace'\n",
    "]\n",
    "\n",
    "encoder = default_french_encoder(vocab_size = 148, cleaners = cleaners)\n",
    "print(encoder)\n",
    "\n",
    "config = {\n",
    "    'nom' : model_name,\n",
    "    'lang' : 'fr',\n",
    "    'text_encoder' : encoder,\n",
    "    'speaker_encoder_name'  : 'audio_siamese_256_mel_lstm',\n",
    "    'speaker_embedding_dim' : 256,\n",
    "    'use_utterance_embedding' : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SV2TTSTacotron2.build_from_nvidia_pretrained(** config)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SV2TTSTacotron2.build_from_pretrained(pretrained_name = 'tacotron2_siwis', ** config)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_name = 'sv2tts_tacotron2_256'\n",
    "\n",
    "model = SV2TTSTacotron2.build_from_sv2tts_pretrained(\n",
    "    pretrained_name = pretrained_name, ** config\n",
    ")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization + dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restoration...\n",
      "Initializing submodel : tts_model !\n",
      "Successfully restored tts_model from pretrained_models/sv2tts_tacotron2_256/saving/tts_model.json !\n",
      "Model sv2tts_tacotron2_256 initialized successfully !\n",
      "Optimizer 'tts_model_optimizer' initilized successfully !\n",
      "Submodel tts_model compiled !\n",
      "  Loss : {'reduction': 'none', 'name': 'tacotron_loss', 'mel_loss': 'mse', 'mask_mel_padding': True, 'label_smoothing': 0, 'finish_weight': 1.0, 'not_finish_weight': 1.0, 'from_logits': False}\n",
      "  Optimizer : {'name': 'Adam', 'learning_rate': 0.0005, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
      "  Metrics : []\n",
      "\n",
      "========== sv2tts_tacotron2_256 ==========\n",
      "Sub model tts_model\n",
      "- Inputs \t: unknown\n",
      "- Outputs \t: unknown\n",
      "- Number of layers \t: 3\n",
      "- Number of parameters \t: 30.341 Millions\n",
      "- Optimizer \t: {'name': 'Adam', 'learning_rate': 0.0005, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
      "- Loss \t : {'reduction': 'none', 'name': 'tacotron_loss', 'mel_loss': 'mse', 'mask_mel_padding': True, 'label_smoothing': 0, 'finish_weight': 1.0, 'not_finish_weight': 1.0, 'from_logits': False}\n",
      "- Metrics\t : []\n",
      "\n",
      "Transfer-learning from : tacotron2_siwis\n",
      "Already trained on 0 epochs (0 steps)\n",
      "\n",
      "Input language : fr\n",
      "Input vocab (size = 148) : ['_', '-', '!', \"'\", '(', ')', ',', '.', ':', ';', '?', ' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'é', 'è', 'ê', 'î', 'ç', 'ô', 'ù', 'ukn_0', 'ukn_1', 'ukn_2', 'ukn_3', 'ukn_4', 'ukn_5', 'ukn_6', 'ukn_7', 'ukn_8', 'ukn_9', 'ukn_10', 'ukn_11', 'ukn_12', 'ukn_13', 'ukn_14', 'ukn_15', 'ukn_16', 'ukn_17', 'ukn_18', 'ukn_19', 'ukn_20', 'ukn_21', 'ukn_22', 'ukn_23', 'ukn_24', 'ukn_25', 'ukn_26', 'ukn_27', 'ukn_28', 'ukn_29', 'ukn_30', 'ukn_31', 'ukn_32', 'ukn_33', 'ukn_34', 'ukn_35', 'ukn_36', 'ukn_37', 'ukn_38', 'ukn_39', 'ukn_40', 'ukn_41', 'ukn_42', 'ukn_43', 'ukn_44', 'ukn_45', 'ukn_46', 'ukn_47', 'ukn_48', 'ukn_49', 'ukn_50', 'ukn_51', 'ukn_52', 'ukn_53', 'ukn_54', 'ukn_55', 'ukn_56', 'ukn_57', 'ukn_58', 'ukn_59', 'ukn_60', 'ukn_61', 'ukn_62', 'ukn_63', 'ukn_64', 'ukn_65', 'ukn_66', 'ukn_67', 'ukn_68', 'ukn_69', 'ukn_70', 'ukn_71', 'ukn_72', 'ukn_73', 'ukn_74', 'ukn_75']\n",
      "Audio rate : 22050\n",
      "Mel channels : 80\n",
      "Speaker embedding dim : 256\n",
      "Speaker encoder : audio_siamese_256_mel_lstm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SV2TTSTacotron2(nom = model_name)\n",
    "\n",
    "lr = { 'name': 'WarmupScheduler', 'maxval' : 75e-5, 'minval' : 25e-5, 'factor' : 1024, 'warmup_steps' : 2048}\n",
    "lr = 5e-4\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam', \n",
    "    optimizer_config = {'lr' : lr}\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset siwis...\n",
      "Loading dataset voxforge...\n",
      "Loading dataset common_voice...\n",
      "Dataset length : 179320 (3847 speakers)\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    'embedding_dim'  : model.speaker_embedding_dim,\n",
    "    'embedding_name' : 'embeddings_256_mel_lstm.csv'\n",
    "}\n",
    "\n",
    "datasets = ['siwis', 'voxforge', 'common_voice']\n",
    "\n",
    "dataset = get_dataset(\n",
    "    datasets, accent = 'france', shuffle = True, ** kwargs\n",
    ")\n",
    "# If the dataset is a dict, it is typically a STT dataset where\n",
    "# validation set contains \"new speakers\" but it is not relevant for this model\n",
    "# because generalization to new speakers is really hard and only interesting if datasets\n",
    "# have many speakers (>> 1k) (which is really rare in TTS datasets)\n",
    "if isinstance(dataset, dict): dataset = dataset['train']\n",
    "#dataset = filter_dataset(dataset, duree = lambda d: d < 11)\n",
    "\n",
    "print(\"Dataset length : {} ({} speakers)\".format(\n",
    "    len(dataset), len(dataset['id'].unique())\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model requires `text`, `filename` and `embedding` (or `speaker_embedding`) columns. The `wavs_22050` allows to directly load resampled audio (which speeds up training). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>filename</th>\n",
       "      <th>time</th>\n",
       "      <th>wavs_16000</th>\n",
       "      <th>wavs_22050</th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "      <th>speaker_embedding</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90694</th>\n",
       "      <td>Vous pouvez te trouver.</td>\n",
       "      <td>D:/datasets/CommonVoice\\clips\\common_voice_fr_...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>D:/datasets/CommonVoice\\wavs_16000\\common_voic...</td>\n",
       "      <td>D:/datasets/CommonVoice\\wavs_22050\\common_voic...</td>\n",
       "      <td>a41aa067e10f026359abc4e901e39bd5e36e49ae0e240c...</td>\n",
       "      <td>[0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...</td>\n",
       "      <td>common_voice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170965</th>\n",
       "      <td>C'est un album contenant uniquement des repris...</td>\n",
       "      <td>D:/datasets/CommonVoice\\clips\\common_voice_fr_...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>D:/datasets/CommonVoice\\wavs_16000\\common_voic...</td>\n",
       "      <td>D:/datasets/CommonVoice\\wavs_22050\\common_voic...</td>\n",
       "      <td>a7c881996f1d72a5bd09e3dcc8aa9994b83ec84a415925...</td>\n",
       "      <td>[0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...</td>\n",
       "      <td>common_voice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102373</th>\n",
       "      <td>Rue Lachaume, dix-neuf, cent Brive-la-Gaillarde</td>\n",
       "      <td>D:/datasets/CommonVoice\\clips\\common_voice_fr_...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>D:/datasets/CommonVoice\\wavs_16000\\common_voic...</td>\n",
       "      <td>D:/datasets/CommonVoice\\wavs_22050\\common_voic...</td>\n",
       "      <td>03c2a68a87a236424f4e404a953b7e0ffde9d69c676552...</td>\n",
       "      <td>[0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...</td>\n",
       "      <td>common_voice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78059</th>\n",
       "      <td>Il joue un seul match en championnat lors de s...</td>\n",
       "      <td>D:/datasets/CommonVoice\\clips\\common_voice_fr_...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>D:/datasets/CommonVoice\\wavs_16000\\common_voic...</td>\n",
       "      <td>D:/datasets/CommonVoice\\wavs_22050\\common_voic...</td>\n",
       "      <td>cf1bac9c9f0549e3476b8a7b742034ee9b59dbb7bb381b...</td>\n",
       "      <td>[0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...</td>\n",
       "      <td>common_voice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91435</th>\n",
       "      <td>Faut-il pas le voler, cet homme, pour fêter v...</td>\n",
       "      <td>D:/datasets/CommonVoice\\clips\\common_voice_fr_...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>D:/datasets/CommonVoice\\wavs_16000\\common_voic...</td>\n",
       "      <td>D:/datasets/CommonVoice\\wavs_22050\\common_voic...</td>\n",
       "      <td>18b38d3fa8c1d0ad4e9174cf8b3b99e7d3c1585ed70aff...</td>\n",
       "      <td>[0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...</td>\n",
       "      <td>common_voice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "90694                             Vous pouvez te trouver.   \n",
       "170965  C'est un album contenant uniquement des repris...   \n",
       "102373    Rue Lachaume, dix-neuf, cent Brive-la-Gaillarde   \n",
       "78059   Il joue un seul match en championnat lors de s...   \n",
       "91435   Faut-il pas le voler, cet homme, pour fêter v...   \n",
       "\n",
       "                                                 filename  time  \\\n",
       "90694   D:/datasets/CommonVoice\\clips\\common_voice_fr_...  -1.0   \n",
       "170965  D:/datasets/CommonVoice\\clips\\common_voice_fr_...  -1.0   \n",
       "102373  D:/datasets/CommonVoice\\clips\\common_voice_fr_...  -1.0   \n",
       "78059   D:/datasets/CommonVoice\\clips\\common_voice_fr_...  -1.0   \n",
       "91435   D:/datasets/CommonVoice\\clips\\common_voice_fr_...  -1.0   \n",
       "\n",
       "                                               wavs_16000  \\\n",
       "90694   D:/datasets/CommonVoice\\wavs_16000\\common_voic...   \n",
       "170965  D:/datasets/CommonVoice\\wavs_16000\\common_voic...   \n",
       "102373  D:/datasets/CommonVoice\\wavs_16000\\common_voic...   \n",
       "78059   D:/datasets/CommonVoice\\wavs_16000\\common_voic...   \n",
       "91435   D:/datasets/CommonVoice\\wavs_16000\\common_voic...   \n",
       "\n",
       "                                               wavs_22050  \\\n",
       "90694   D:/datasets/CommonVoice\\wavs_22050\\common_voic...   \n",
       "170965  D:/datasets/CommonVoice\\wavs_22050\\common_voic...   \n",
       "102373  D:/datasets/CommonVoice\\wavs_22050\\common_voic...   \n",
       "78059   D:/datasets/CommonVoice\\wavs_22050\\common_voic...   \n",
       "91435   D:/datasets/CommonVoice\\wavs_22050\\common_voic...   \n",
       "\n",
       "                                                       id  \\\n",
       "90694   a41aa067e10f026359abc4e901e39bd5e36e49ae0e240c...   \n",
       "170965  a7c881996f1d72a5bd09e3dcc8aa9994b83ec84a415925...   \n",
       "102373  03c2a68a87a236424f4e404a953b7e0ffde9d69c676552...   \n",
       "78059   cf1bac9c9f0549e3476b8a7b742034ee9b59dbb7bb381b...   \n",
       "91435   18b38d3fa8c1d0ad4e9174cf8b3b99e7d3c1585ed70aff...   \n",
       "\n",
       "                                                embedding  \\\n",
       "90694   [0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...   \n",
       "170965  [0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...   \n",
       "102373  [0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...   \n",
       "78059   [0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...   \n",
       "91435   [0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...   \n",
       "\n",
       "                                        speaker_embedding       dataset  \n",
       "90694   [0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...  common_voice  \n",
       "170965  [0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...  common_voice  \n",
       "102373  [0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...  common_voice  \n",
       "78059   [0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...  common_voice  \n",
       "91435   [0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0...  common_voice  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs          = 5\n",
    "batch_size      = 32\n",
    "valid_batch_size    = batch_size\n",
    "\n",
    "max_valid_size  = min(int(0.1 * len(dataset)), 256 * valid_batch_size)\n",
    "\n",
    "train_size      = min(1024 * batch_size, len(dataset) - max_valid_size)\n",
    "valid_size      = min(len(dataset) - train_size, max_valid_size)\n",
    "\n",
    "shuffle_size    = batch_size * 12\n",
    "pred_step       = 512\n",
    "\n",
    "\"\"\" Custom training hparams \"\"\"\n",
    "augment_prct        = 0.25\n",
    "augment_speaker_embedding   = False\n",
    "\n",
    "trim_audio      = True\n",
    "reduce_noise    = True if 'common_voice' in datasets else False\n",
    "trim_threshold  = 0.075\n",
    "max_silence     = 0.1\n",
    "trim_method     = 'window'\n",
    "trim_mode       = 'start_end'\n",
    "\n",
    "trim_mel     = False\n",
    "trim_factor  = 0.6\n",
    "trim_mel_method  = 'max_start_end'\n",
    "\n",
    "# Seems to be interesting for single-speaker fine-tuning\n",
    "# and for a better generalization but seems to slow down convergence \n",
    "use_utterance_embedding = True\n",
    "\n",
    "max_input_length = 75\n",
    "max_output_length = 512\n",
    "\n",
    "\"\"\" Training \"\"\"\n",
    "\n",
    "train, valid = train_test_split(\n",
    "    dataset, train_size = train_size, valid_size = valid_size, shuffle = True\n",
    ")\n",
    "\n",
    "print(\"Training samples   : {} - {} batches - {} speakers\".format(\n",
    "    len(train), len(train) // batch_size, len(train['id'].unique())\n",
    "))\n",
    "print(\"Validation samples : {} - {} batches - {} speakers\".format(\n",
    "    len(valid), len(valid) // valid_batch_size, len(valid['id'].unique())\n",
    "))\n",
    "\n",
    "# This feature seems interesting for singl-speaker fine-tuning\n",
    "# If you want to enable it, put `trainable = False`\n",
    "trainable = False\n",
    "if model.epochs >= 5:\n",
    "    model.tts_model.postnet.trainable = trainable\n",
    "if model.epochs >= 10:\n",
    "    model.tts_model.encoder.trainable = trainable\n",
    "\n",
    "model.train(\n",
    "    train, validation_data = valid, \n",
    "    epochs = epochs, batch_size = batch_size, valid_batch_size = valid_batch_size,\n",
    "\n",
    "    max_input_length = max_input_length, max_output_length = max_output_length,\n",
    "    shuffle_size = shuffle_size, pred_step = pred_step,\n",
    "    augment_prct = augment_prct, augment_speaker_embedding = augment_speaker_embedding,\n",
    "    \n",
    "    trim_audio = trim_audio, reduce_noise = reduce_noise, trim_threshold = trim_threshold,\n",
    "    max_silence = max_silence, trim_method = trim_method, trim_mode = trim_mode,\n",
    "    \n",
    "    trim_mel = trim_mel, trim_factor = trim_factor, trim_mel_method = trim_mel_method,\n",
    "    \n",
    "    use_utterance_embedding = use_utterance_embedding\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_history()\n",
    "print(model.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction / inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with target\n",
    "\n",
    "This cell allows you to make prediction **with** target : it means the model will have an inference and a prediction (as during training)\n",
    "\n",
    "Next you can make the `waveglow inference` with the section related to `prediction on training generated` (you just have to replace `mode = 'train'` by `mode = 'pred'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.get_dataset_config(batch_size = 1, cache = False, is_validation = True)\n",
    "\n",
    "sub_ds = valid.sample(15, random_state = 1024)\n",
    "ds = prepare_dataset(sub_ds, ** config)\n",
    "\n",
    "for i, batch in enumerate(ds):\n",
    "    model.predict_with_target(\n",
    "        batch, step = 'test', prefix = 'pred_{}'.format(i),\n",
    "        directory = model.pred_dir, show = True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction without target\n",
    "\n",
    "This cell will allow you to make prediction **without** target (ie by putting the text you want) !\n",
    "\n",
    "Note that this 1st cell is to set the default embeddings to use in prediction : by default, it samples the training dataset to define default embeddings. These default embeddings are the one used when prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'embedding_dim'  : model.speaker_embedding_dim,\n",
    "    'embedding_name' : 'embeddings_256_mel_lstm.csv'\n",
    "}\n",
    "\n",
    "datasets = 'siwis'\n",
    "\n",
    "dataset = get_dataset(datasets, ** kwargs)\n",
    "\n",
    "embeddings = dataset.sample(10)\n",
    "model.set_default_embeddings(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Bonjour le monde !\"\n",
    "\n",
    "model.predict(text, overwrite = True, debug = True, save = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete inference\n",
    "\n",
    "These cells allow you to test your model with a complete inference pipeline\n",
    "\n",
    "Note that if you want to use `PtWaveGlow` instead of `WaveGlow`, you have to restart your kernel then execute 1st cell (imports) then cells below. You **must** first instanciate the `PtWaveGlow` model which will call `limit_gpu_memory` to reduce visible GPU memory for tensorflow (to allow a better coexistance of both libraries). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveglow = WaveGlow() #PtWaveGlow()\n",
    "model    = SV2TTSTacotron2(nom = model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_inference(text, embedding, n = 1):\n",
    "    encoded = tf.expand_dims(model.encode_text(text), axis = 0)\n",
    "    \n",
    "    _, mel, _, attn = model.infer(\n",
    "        encoded, [tf.shape(encoded)[1]], embedding\n",
    "    )\n",
    "    \n",
    "    plot_spectrogram(inference = mel[0], attention = attn)\n",
    "    audio = waveglow.infer(mel)\n",
    "\n",
    "    display_audio(audio, rate = rate)\n",
    "    return audio\n",
    "\n",
    "kwargs = {\n",
    "    'embedding_dim'  : model.speaker_embedding_dim,\n",
    "    'embedding_name' : 'embeddings_256_mel_lstm.csv'\n",
    "}\n",
    "\n",
    "datasets = ['siwis', 'voxforge']\n",
    "\n",
    "dataset = get_dataset(\n",
    "    datasets, accent = 'france', shuffle = True, ** kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Bonjour tout le monde ! Voici une démonstration du modèle en français.\"\n",
    "\n",
    "# Select an embedding and display original speaker audio associated to this embedding\n",
    "x = random.randrange(0, len(dataset))\n",
    "display_audio(dataset.at[x, 'filename'])\n",
    "embedding = select_embedding(dataset, mode = x)\n",
    "\n",
    "silence = np.zeros((int(rate * 0.15),))\n",
    "audios = []\n",
    "if not isinstance(text, list): text = [text]\n",
    "\n",
    "for p in text:\n",
    "    for _ in range(2):\n",
    "        audio = full_inference(p, embedding)\n",
    "    audios.append(audio)\n",
    "    audios.append(silence)\n",
    "\n",
    "if len(text) > 1:\n",
    "    audios = np.concatenate(audios)\n",
    "    _ = display_audio(audios, rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waveglow inference on training generated audios\n",
    "\n",
    "This is a demonstration on prediction at step 500 so it is normal that inference is so bad ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_target(model_name, step, n, mode, save = False, display = True):\n",
    "    if mode == 'train':\n",
    "        directory = os.path.join('pretrained_models', model_name, 'training-logs', 'eval', 'mels')\n",
    "        filename = 'pred_step-{:06d}_{}_target.npy'.format(step, n)\n",
    "        pred_filename = 'pred_step-{:06d}_{}_pred.npy'.format(step, n)\n",
    "        infer_filename = 'pred_step-{:06d}_{}_infer.npy'.format(step, n)\n",
    "    else:\n",
    "        directory = os.path.join('pretrained_models', model_name, 'outputs', 'mels')\n",
    "        filename = 'pred_{}_target.npy'.format(n)\n",
    "        pred_filename = 'pred_{}_pred.npy'.format(n)\n",
    "        infer_filename = 'pred_{}_infer.npy'.format(n)\n",
    "\n",
    "    if not os.path.exists(os.path.join(directory, filename)): return\n",
    "    \n",
    "    target = np.load(os.path.join(directory, filename))\n",
    "    pred   = np.load(os.path.join(directory, pred_filename))\n",
    "    infer  = np.load(os.path.join(directory, infer_filename))\n",
    "    \n",
    "    audio       = waveglow.infer(target)\n",
    "    audio_pred  = waveglow.infer(pred)\n",
    "    audio_infer = waveglow.infer(infer)\n",
    "\n",
    "    _ = display_audio(audio, rate = rate)\n",
    "    _ = display_audio(audio_pred, rate = rate)\n",
    "    _ = display_audio(audio_infer, rate = rate)\n",
    "    \n",
    "    if save:\n",
    "        save_dir = directory.replace('mels', 'audios')\n",
    "        os.makedirs(save_dir, exist_ok = True)\n",
    "        write_audio(audio, os.path.join(save_dir, filename[:-3] + 'mp3'), rate = rate)\n",
    "        write_audio(audio_pred, os.path.join(save_dir, pred_filename[:-3] + 'mp3'), rate = rate)\n",
    "        write_audio(audio_infer, os.path.join(save_dir, infer_filename[:-3] + 'mp3'), rate = rate)\n",
    "    \n",
    "    plot_spectrogram(\n",
    "        target = target, prediction = pred, inference = infer\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, mode = 500, 'train'\n",
    "\n",
    "for n in range(5):\n",
    "    infer_with_target(model_name, step, n, mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_train_objects.optimizers import WarmupScheduler\n",
    "\n",
    "lr = WarmupScheduler(maxval = 75e-5, minval = 25e-5, factor = 1024)\n",
    "print(lr.get_config())\n",
    "lr.plot(1024 * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
